diff -urN old/slob.c new/slob.c
--- old/slob.c	2015-11-27 13:43:33.160725115 -0800
+++ new/slob.c	2015-11-27 13:39:53.289218529 -0800
@@ -67,9 +67,10 @@
 #include <linux/rcupdate.h>
 #include <linux/list.h>
 #include <linux/kmemleak.h>
-
+#include <linux/syscalls.h>
 #include <trace/events/kmem.h>
 
+#include <linux/linkage.h>
 #include <linux/atomic.h>
 
 #include "slab.h"
@@ -87,6 +88,9 @@
 typedef s32 slobidx_t;
 #endif
 
+unsigned long page_count_slob = 0; 
+unsigned long free_units = 0;
+
 struct slob_block {
 	slobidx_t units;
 };
@@ -133,7 +137,6 @@
 	struct rcu_head head;
 	int size;
 };
-
 /*
  * slob_lock protects all slob allocator structures.
  */
@@ -268,10 +271,13 @@
 static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 {
 	struct page *sp;
+    struct page *sp_other = NULL;
 	struct list_head *prev;
 	struct list_head *slob_list;
+    struct list_head *temp;
 	slob_t *b = NULL;
 	unsigned long flags;
+    free_units = 0;
 
 	if (size < SLOB_BREAK1)
 		slob_list = &free_slob_small;
@@ -295,20 +301,36 @@
 		if (sp->units < SLOB_UNITS(size))
 			continue;
 
-		/* Attempt to alloc */
-		prev = sp->list.prev;
-		b = slob_page_alloc(sp, size, align);
-		if (!b)
-			continue;
+        if(sp_other == NULL)
+                sp_other = sp;
 
-		/* Improve fragment distribution and reduce our average
-		 * search time by starting our next search here. (see
-		 * Knuth vol 1, sec 2.5, pg 449) */
-		if (prev != slob_list->prev &&
-				slob_list->next != prev->next)
-			list_move_tail(slob_list, prev->next);
-		break;
+        //Make sure we get the smallest page for what we need
+        if(sp->units < sp_other->units)
+                sp_other = sp;
 	}
+
+    //Now try to allocate
+    if(sp_other != NULL)
+    {
+        b = slob_page_alloc(sp_other, size, align);
+    }
+
+    //Loop through each linked list to find free space
+    temp = &free_slob_small;
+    list_for_each_entry(sp, temp, list) {
+        free_units += sp->units;
+    }
+    temp = &free_slob_medium;
+    list_for_each_entry(sp, temp, list) { 
+        free_units += sp->units;
+    }
+    temp = &free_slob_large;
+    list_for_each_entry(sp, temp, list) {
+        free_units += sp->units;
+
+    }
+
+
 	spin_unlock_irqrestore(&slob_lock, flags);
 
 	/* Not enough space: must allocate a new page */
@@ -328,6 +350,8 @@
 		b = slob_page_alloc(sp, size, align);
 		BUG_ON(!b);
 		spin_unlock_irqrestore(&slob_lock, flags);
+        //Allocated a new page
+        page_count_slob++;
 	}
 	if (unlikely((gfp & __GFP_ZERO) && b))
 		memset(b, 0, size);
@@ -362,6 +386,8 @@
 		__ClearPageSlab(sp);
 		page_mapcount_reset(sp);
 		slob_free_pages(b, 0);
+        //Freed page, decremeount page count
+        page_count_slob--;
 		return;
 	}
 
@@ -633,6 +659,19 @@
 	.align = ARCH_KMALLOC_MINALIGN,
 };
 
+asmlinkage long sys_slob_used(void) {
+
+    //used = page size * number of pages
+    long slob_total_used = SLOB_UNITS(PAGE_SIZE) * page_count_slob;
+
+    return slob_total_used;
+}
+
+asmlinkage long sys_slob_free(void) {
+
+    return free_units;
+}
+
 void __init kmem_cache_init(void)
 {
 	kmem_cache = &kmem_cache_boot;
diff -urN old/syscall_32.tbl new/syscall_32.tbl
--- old/syscall_32.tbl	2015-11-27 13:43:26.838624278 -0800
+++ new/syscall_32.tbl	2015-11-27 13:40:27.170758834 -0800
@@ -359,3 +359,5 @@
 350	i386	finit_module		sys_finit_module
 351	i386	sched_setattr		sys_sched_setattr
 352	i386	sched_getattr		sys_sched_getattr
+353 i386    slob_free           sys_slob_free
+354 i386    slob_used           sys_slob_used
diff -urN old/syscalls.h new/syscalls.h
--- old/syscalls.h	2015-11-27 13:43:16.711462751 -0800
+++ new/syscalls.h	2015-11-27 13:41:09.372431846 -0800
@@ -855,4 +855,6 @@
 asmlinkage long sys_kcmp(pid_t pid1, pid_t pid2, int type,
 			 unsigned long idx1, unsigned long idx2);
 asmlinkage long sys_finit_module(int fd, const char __user *uargs, int flags);
+asmlinkage long sys_slob_free(void);
+asmlinkage long sys_slob_used(void);
 #endif
